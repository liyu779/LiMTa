defaults:
  - _self_
  - wandb: private.yaml
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled
tensorboard: 
  enabled: True
# disable hydra outputs
hydra:
  output_subdir: null
  run:
    dir: .

name: "traffic-adapter"
pretrained_feature_extractor: "/home/model-server/code/LiMTa/trained_models/pretrain-256/traffic-256.ckpt"
backbone:
  name: "traffic_encoder_simple" # traffic_encoder_simple traffic_transformer
  kwargs:
    input_dim: 500
    d_model: 256 # hidden layer dimensions
    seq_length : 400
    n_head : 4
    kernel_size : 3
    n_layer : 6
    out_dim: 256
    # LoRA parameters
    r : 32 
    alpha: 64  # note. rank mast alpha to keep same scaling
    drop : 0.2
    lora_n_layer: 6

embeding:
  dim : 500
pretrain_method: "none"
data:
  dataset: USTCTFC2016 # change here for cifar100 ISCXTor2016 CrossPlatform-Android
  train_path: "/home/model-server/code/dataset/USTCTFC2016"
  val_path: "/home/model-server/code/dataset/USTCTFC2016"
  format: "image_folder"
  num_workers: 4
  data_fraction: -1 # adjust data fraction part of dataset to use

optimizer:
  name: "sgd"
  batch_size: 256
  lr: 0.05
  weight_decay: 0
scheduler:
  name: "step"
  lr_decay_steps: [60, 80]
checkpoint:
  enabled: False
  dir: "trained_models"
  frequency: 1
auto_resume:
  enabled: True
finetune: False
# overwrite PL stuff
max_epochs: 100
devices: [0]
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp"
precision: 16
